{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Video Classification using CNN-RNN\n\nThe script defines and train a video classification model that uses a Convolutional Neural Network (CNN) and a Recurrent Neural Network (RNN) consisting of GRU layers. Data used for training is extracted from the UCF101 dataset. The videos are categorized into different actions, like cricket shot, punching, biking, etc. A video consists of an ordered sequence of frames. Each frame contains spatial information, and the sequence of those frames contains temporal information. This is model by the hybrid CNN-RNN architecture where the convolutions defines the spatial processing while the RNN defines the temporal processing. Specifically, we use the GRU layers for the Recurrent Neural Network (RNN).","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install -q git+https://github.com/tensorflow/docs","metadata":{"execution":{"iopub.status.busy":"2021-10-24T16:32:22.755525Z","iopub.execute_input":"2021-10-24T16:32:22.755825Z","iopub.status.idle":"2021-10-24T16:32:45.999857Z","shell.execute_reply.started":"2021-10-24T16:32:22.755782Z","shell.execute_reply":"2021-10-24T16:32:45.998711Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from tensorflow_docs.vis import embed\nfrom tensorflow import keras\n#from imutils import paths\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport imageio\nimport cv2\nimport os","metadata":{"execution":{"iopub.status.busy":"2021-10-24T16:32:46.002187Z","iopub.execute_input":"2021-10-24T16:32:46.002443Z","iopub.status.idle":"2021-10-24T16:32:46.008071Z","shell.execute_reply.started":"2021-10-24T16:32:46.002409Z","shell.execute_reply":"2021-10-24T16:32:46.006974Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation\n\nIn order to keep the runtime of this example relatively short, we will be using a subsampled version of the original UCF101 dataset. You can refer to this [notebook](https://colab.research.google.com/github/sayakpaul/Action-Recognition-in-TensorFlow/blob/main/Data_Preparation_UCF101.ipynb) to know how the subsampling was done.","metadata":{}},{"cell_type":"code","source":"IMG_SIZE = 224\nBATCH_SIZE = 64\nEPOCHS = 10\n\nMAX_SEQ_LENGTH = 40\nNUM_FEATURES = 2048","metadata":{"execution":{"iopub.status.busy":"2021-10-24T16:32:50.899132Z","iopub.execute_input":"2021-10-24T16:32:50.899411Z","iopub.status.idle":"2021-10-24T16:32:50.905010Z","shell.execute_reply.started":"2021-10-24T16:32:50.899382Z","shell.execute_reply":"2021-10-24T16:32:50.903971Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/ucf101/train.csv\")\nvalidate_df = pd.read_csv(\"/kaggle/input/ucf101/validate.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/ucf101/test.csv\")\n\nprint(\"Total videos for training: {}\".format(len(train_df)))\nprint(\"Total videos for validate: {}\".format(len(validate_df)))\nprint(\"Total videos for testing: {}\".format(len(test_df)))\n\ntrain_df.sample(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T16:32:59.257849Z","iopub.execute_input":"2021-10-24T16:32:59.258135Z","iopub.status.idle":"2021-10-24T16:32:59.279896Z","shell.execute_reply.started":"2021-10-24T16:32:59.258105Z","shell.execute_reply":"2021-10-24T16:32:59.279003Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"Count the number of instances for the training and test sets","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\npd.DataFrame.from_dict(\n    {'train_tags'      : Counter(train_df[\"tag\"]).keys(), \n     'train_counts'    : Counter(train_df[\"tag\"]).values(),\n     'validate_tags'   : Counter(validate_df[\"tag\"]).keys(),\n     'validate_counts' : Counter(validate_df[\"tag\"]).values(),\n     'test_tags'       : Counter(test_df[\"tag\"]).keys(),\n     'test_counts'     : Counter(test_df[\"tag\"]).values()})","metadata":{"execution":{"iopub.status.busy":"2021-10-24T16:33:11.941557Z","iopub.execute_input":"2021-10-24T16:33:11.942029Z","iopub.status.idle":"2021-10-24T16:33:11.958289Z","shell.execute_reply.started":"2021-10-24T16:33:11.941995Z","shell.execute_reply":"2021-10-24T16:33:11.957252Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"Since a video is an ordered sequence of frames, we could just extract the frames and put them in a 3D tensor. But the number of frames is different across videos which will prevents us from stacking them nicely into fixed size batches (unless we use padding). In this implementation, we save all the video frames until a maximum frame count is reached. Specifically:\n\n1. Extract frames from the videos\n2. Center crop andd resize each frame \n3. Append the processed frames until the maximum frame count is reached.\n\nIn the case, where a video's frame count is lesser than the maximum frame count we will pad the video with zeros.\nNote that this workflow is identical to problems involving texts sequences. Videos of the UCF101 dataset is known to not contain extreme variations in objects and actions across frames. Because of this, it may be okay to only consider a few frames for the learning task. But this approach may not generalize well to other video classification problems. We will be using OpenCV's VideoCapture() method to read frames from videos.","metadata":{}},{"cell_type":"code","source":"# The following two methods are taken from this tutorial:\n# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n\ndef crop_center_square(frame):\n    y, x = frame.shape[0:2]\n    min_dim = min(y, x)\n    start_x = (x // 2) - (min_dim // 2)\n    start_y = (y // 2) - (min_dim // 2)\n    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n\n\ndef load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n    cap = cv2.VideoCapture(path)\n    frames = []\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = crop_center_square(frame)\n            frame = cv2.resize(frame, resize)\n            frame = frame[:, :, [2, 1, 0]]\n            frames.append(frame)\n\n            if len(frames) == max_frames:\n                break\n    finally:\n        cap.release()\n    return np.array(frames)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T16:33:18.186460Z","iopub.execute_input":"2021-10-24T16:33:18.186771Z","iopub.status.idle":"2021-10-24T16:33:18.198784Z","shell.execute_reply.started":"2021-10-24T16:33:18.186739Z","shell.execute_reply":"2021-10-24T16:33:18.197854Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# Feature extraction using InceptionV3\n\nWe can use a pre-trained InceptionV3 (pre-trained using ImageNet-1k) to extract the features from each video frames.","metadata":{}},{"cell_type":"code","source":"def build_feature_extractor():\n    feature_extractor = keras.applications.InceptionV3(\n        weights=\"imagenet\",\n        include_top=False,\n        pooling=\"avg\",\n        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n    )\n    preprocess_input = keras.applications.inception_v3.preprocess_input\n\n    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n    preprocessed = preprocess_input(inputs)\n\n    outputs = feature_extractor(preprocessed)\n    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n\nfeature_extractor = build_feature_extractor()","metadata":{"execution":{"iopub.status.busy":"2021-10-24T16:33:20.498513Z","iopub.execute_input":"2021-10-24T16:33:20.498816Z","iopub.status.idle":"2021-10-24T16:33:24.361393Z","shell.execute_reply.started":"2021-10-24T16:33:20.498782Z","shell.execute_reply":"2021-10-24T16:33:24.360541Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"The labels of the videos are strings and cannot be processed by the neural networks. So they are mapped to numerical value before they are fed to the model for training. This is done using the StringLookup layer encode the class string labels into integers.","metadata":{}},{"cell_type":"code","source":"label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]))\n\nprint(label_processor.get_vocabulary())","metadata":{"execution":{"iopub.status.busy":"2021-10-24T16:33:24.363339Z","iopub.execute_input":"2021-10-24T16:33:24.363639Z","iopub.status.idle":"2021-10-24T16:33:24.373032Z","shell.execute_reply.started":"2021-10-24T16:33:24.363595Z","shell.execute_reply":"2021-10-24T16:33:24.372254Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def prepare_all_videos(df, root_dir):\n    num_samples = len(df)\n    processed_count = 0\n    video_paths = df[\"video_name\"].values.tolist()\n    labels = df[\"tag\"].values\n    labels = label_processor(labels[..., None]).numpy()\n\n    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n    # masked with padding or not.\n    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n\n    # For each video.\n    for idx, path in enumerate(video_paths):\n        # Gather all its frames and add a batch dimension.\n        frames = load_video(os.path.join(root_dir, path))\n        frames = frames[None, ...]\n\n        # Initialize placeholders to store the masks and features of the current video.\n        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n        temp_frame_features = np.zeros(\n            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n        )\n\n        # Extract features from the frames of the current video.\n        for i, batch in enumerate(frames):\n            video_length = batch.shape[0]\n            length = min(MAX_SEQ_LENGTH, video_length)\n            for j in range(length):\n                temp_frame_features[i, j, :] = feature_extractor.predict(\n                    batch[None, j, :]\n                )\n            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n\n        frame_features[idx,] = temp_frame_features.squeeze()\n        frame_masks[idx,] = temp_frame_mask.squeeze()\n\n        processed_count += 1\n        print(\"{}/{} videos done\".format(processed_count, num_samples), end = \"\\r\")\n    \n    print(\"\\n\")\n\n    return (frame_features, frame_masks), labels","metadata":{"execution":{"iopub.status.busy":"2021-10-24T16:33:26.461901Z","iopub.execute_input":"2021-10-24T16:33:26.462211Z","iopub.status.idle":"2021-10-24T16:33:26.475243Z","shell.execute_reply.started":"2021-10-24T16:33:26.462176Z","shell.execute_reply":"2021-10-24T16:33:26.474304Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# print(\"Preparing train videos:\")\n# train_data, train_labels = prepare_all_videos(train_df.loc[[0,1,2,3]], \"/kaggle/input/ucf101/train/\")\n# print(\"Preparing validate videos\")\n# validate_data, validate_labels = prepare_all_videos(validate_df.loc[[0,1]], \"/kaggle/input/ucf101/validate/\")\n# print(\"Preparing test videos:\")\n# test_data, test_labels = prepare_all_videos(test_df.loc[[0,1]], \"/kaggle/input/ucf101/test/\")\n\nprint(\"Preparing train videos\")\ntrain_data, train_labels = prepare_all_videos(train_df, \"/kaggle/input/ucf101/train/\")\nprint(\"Preparing validate videos\")\nvalidate_data, validate_labels = prepare_all_videos(validate_df, \"/kaggle/input/ucf101/validate/\")\nprint(\"Preparing test videos\")\ntest_data, test_labels = prepare_all_videos(test_df, \"/kaggle/input/ucf101/test/\")\n\nprint(\"Frame features in train set: {}\".format(train_data[0].shape))\nprint(\"Frame masks in train set: {}\".format(train_data[1].shape))","metadata":{"execution":{"iopub.status.busy":"2021-10-24T16:33:29.382041Z","iopub.execute_input":"2021-10-24T16:33:29.382332Z","iopub.status.idle":"2021-10-24T16:34:15.037941Z","shell.execute_reply.started":"2021-10-24T16:33:29.382304Z","shell.execute_reply":"2021-10-24T16:34:15.036959Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"# Define Sequence Model","metadata":{}},{"cell_type":"code","source":"# Utility for our sequence model.\ndef get_sequence_model():\n    class_vocab = label_processor.get_vocabulary()\n\n    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n\n    x = keras.layers.GRU(32, return_sequences=True)(frame_features_input, mask=mask_input)\n    #x = keras.layers.GRU(16, return_sequences=True)(x)\n    x = keras.layers.GRU(16)(x)\n    x = keras.layers.Dropout(0.25)(x)\n    x = keras.layers.Dense(16, activation=\"relu\")(x)\n    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n\n    rnn_model = keras.Model([frame_features_input, mask_input], output)\n\n    rnn_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n    return rnn_model","metadata":{"execution":{"iopub.status.busy":"2021-10-24T16:34:15.040130Z","iopub.execute_input":"2021-10-24T16:34:15.040456Z","iopub.status.idle":"2021-10-24T16:34:15.049137Z","shell.execute_reply.started":"2021-10-24T16:34:15.040413Z","shell.execute_reply":"2021-10-24T16:34:15.048161Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"filepath = \"video_classifier.h5\"\ncheckpoint = keras.callbacks.ModelCheckpoint(filepath, \n                                             save_weights_only=True, \n                                             save_best_only=True, \n                                             verbose=1)\n\nseq_model = get_sequence_model()\n    \nhistory = seq_model.fit(x = [train_data[0], train_data[1]],\n                        y = train_labels,\n                        validation_data = ([validate_data[0], validate_data[1]], validate_labels),\n                        epochs=EPOCHS,\n                        callbacks=[checkpoint])","metadata":{"execution":{"iopub.status.busy":"2021-10-24T16:34:15.050646Z","iopub.execute_input":"2021-10-24T16:34:15.052258Z","iopub.status.idle":"2021-10-24T16:34:28.862597Z","shell.execute_reply.started":"2021-10-24T16:34:15.052211Z","shell.execute_reply":"2021-10-24T16:34:28.861731Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"seq_model.load_weights(filepath)\n\n_, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n\nprint(\"Test accuracy: {}%\".format(round(accuracy * 100, 2)))","metadata":{"execution":{"iopub.status.busy":"2021-10-24T16:34:28.866374Z","iopub.execute_input":"2021-10-24T16:34:28.866641Z","iopub.status.idle":"2021-10-24T16:34:28.974661Z","shell.execute_reply.started":"2021-10-24T16:34:28.866609Z","shell.execute_reply":"2021-10-24T16:34:28.973828Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"def plot_result(item):\n    plt.plot(history.history[item], label=item)\n    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(item)\n    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n    plt.legend()\n    plt.grid()\n    plt.show()\n\nplot_result(\"accuracy\")","metadata":{"execution":{"iopub.status.busy":"2021-10-24T16:34:28.976349Z","iopub.execute_input":"2021-10-24T16:34:28.976711Z","iopub.status.idle":"2021-10-24T16:34:29.205041Z","shell.execute_reply.started":"2021-10-24T16:34:28.976667Z","shell.execute_reply":"2021-10-24T16:34:29.204160Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"def prepare_single_video(frames):\n    frames = frames[None, ...]\n    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n\n    for i, batch in enumerate(frames):\n        video_length = batch.shape[0]\n        length = min(MAX_SEQ_LENGTH, video_length)\n        for j in range(length):\n            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n\n    return frame_features, frame_mask\n\n\ndef sequence_prediction(path):\n    class_vocab = label_processor.get_vocabulary()\n\n    frames = load_video(os.path.join(\"/kaggle/input/ucf101/test/\", path))\n    frame_features, frame_mask = prepare_single_video(frames)\n    probabilities = seq_model.predict([frame_features, frame_mask])[0]\n\n    for i in np.argsort(probabilities)[::-1]:\n        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n    return frames\n\n\n# This utility is for visualization.\n# Referenced from:\n# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\ndef to_gif(images):\n    converted_images = images.astype(np.uint8)\n    imageio.mimsave(\"animation.gif\", converted_images, fps=10)\n    return embed.embed_file(\"animation.gif\")","metadata":{"execution":{"iopub.status.busy":"2021-10-24T12:21:40.790690Z","iopub.execute_input":"2021-10-24T12:21:40.791049Z","iopub.status.idle":"2021-10-24T12:21:40.805679Z","shell.execute_reply.started":"2021-10-24T12:21:40.791013Z","shell.execute_reply":"2021-10-24T12:21:40.804435Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\nprint(\"Test video: {}\".format(test_video))\ntest_frames = sequence_prediction(test_video)\nto_gif(test_frames[:MAX_SEQ_LENGTH])","metadata":{"execution":{"iopub.status.busy":"2021-10-24T12:21:42.757131Z","iopub.execute_input":"2021-10-24T12:21:42.757417Z","iopub.status.idle":"2021-10-24T12:21:56.761164Z","shell.execute_reply.started":"2021-10-24T12:21:42.757380Z","shell.execute_reply":"2021-10-24T12:21:56.760052Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\nprint(\"Test video: {}\".format(test_video))\ntest_frames = sequence_prediction(test_video)\nto_gif(test_frames[:MAX_SEQ_LENGTH])","metadata":{"execution":{"iopub.status.busy":"2021-10-24T12:21:56.763564Z","iopub.execute_input":"2021-10-24T12:21:56.764459Z","iopub.status.idle":"2021-10-24T12:22:08.520967Z","shell.execute_reply.started":"2021-10-24T12:21:56.764421Z","shell.execute_reply":"2021-10-24T12:22:08.519872Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\nprint(\"Test video: {}\".format(test_video))\ntest_frames = sequence_prediction(test_video)\nto_gif(test_frames[:MAX_SEQ_LENGTH])","metadata":{"execution":{"iopub.status.busy":"2021-10-24T12:22:08.524062Z","iopub.execute_input":"2021-10-24T12:22:08.525665Z","iopub.status.idle":"2021-10-24T12:22:19.829431Z","shell.execute_reply.started":"2021-10-24T12:22:08.525593Z","shell.execute_reply":"2021-10-24T12:22:19.827818Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}