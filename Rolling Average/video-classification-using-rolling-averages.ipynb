{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Video Classification using rolling averages","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import matplotlib\nmatplotlib.use(\"Agg\")\n\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport cv2\nimport os\nimport random\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2021-10-26T04:05:44.727167Z","iopub.execute_input":"2021-10-26T04:05:44.727976Z","iopub.status.idle":"2021-10-26T04:05:50.296077Z","shell.execute_reply.started":"2021-10-26T04:05:44.727882Z","shell.execute_reply":"2021-10-26T04:05:50.295365Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation\n\nRead in the csv files and shuffle the dataframe. Each dataframe roll consists of the video file name and the tag. The file names are read in to retrieve the video files. Each video file is then read by using OpenCV to extract the frames at n-th interval. Each frame is centre cropped and resized to (224,224). The processed frames are then appended to form the training data frames. A copy of the frame is also saved as an image in the train_image folder\n\nIn this version, only 5 classes from the UCF101 data sets are used.","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/ucf101-v2/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/ucf101-v2/test.csv\")\n\ntrain_df = train_df.sample(frac = 1)\ntest_df = test_df.sample(frac = 1)\n\nprint(\"Total videos for training: {}\".format(len(train_df)))\nprint(\"Total videos for testing: {}\".format(len(test_df)))\n\n#train_df.sample(10)\ntrain_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T04:05:50.297767Z","iopub.execute_input":"2021-10-26T04:05:50.298012Z","iopub.status.idle":"2021-10-26T04:05:50.498618Z","shell.execute_reply.started":"2021-10-26T04:05:50.297975Z","shell.execute_reply":"2021-10-26T04:05:50.497876Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"IMG_SIZE = 224\n# BATCH_SIZE = 8\nBATCH_SIZE = 32\nEPOCHS = 5\n\ndef crop_center_square(frame):\n    y, x = frame.shape[0:2]\n    min_dim = min(y, x)\n    start_x = (x // 2) - (min_dim // 2)\n    start_y = (y // 2) - (min_dim // 2)\n    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n\n\ndef extract_frames(path, label, skip = 60, resize=(IMG_SIZE, IMG_SIZE)):\n    if not os.path.exists(\"train_images/\"):\n        os.makedirs(\"train_images/\")\n            \n    cap = cv2.VideoCapture(path)\n    image_frames = []\n    image_label = []\n    count = 0\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = crop_center_square(frame)\n            frame = cv2.resize(frame, resize)\n            #frame = frame[:, :, [2, 1, 0]]\n            filename = 'train_images/' + os.path.splitext(os.path.basename(path))[0] + '_' + str(count) + '.jpg'\n            cv2.imwrite(filename, frame)\n            image_frames.append(frame)\n            image_label.append(label)\n            \n            count += skip\n            cap.set(cv2.CAP_PROP_POS_FRAMES, count) #read every 5th frame\n    finally:\n        cap.release()\n    \n    return image_frames, image_label","metadata":{"execution":{"iopub.status.busy":"2021-10-26T04:05:50.500180Z","iopub.execute_input":"2021-10-26T04:05:50.500768Z","iopub.status.idle":"2021-10-26T04:05:50.512415Z","shell.execute_reply.started":"2021-10-26T04:05:50.500717Z","shell.execute_reply":"2021-10-26T04:05:50.511603Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def prepare_data(df, root_dir):\n    num_samples = len(df)\n    all_frames = []\n    all_labels = []\n    processed_count = 0\n    \n    for index, row in df.iterrows():\n        frames, labels = extract_frames(os.path.join(root_dir, row['video_name']), row['tag'])\n        all_frames = all_frames + frames\n        all_labels = all_labels + labels\n        processed_count += 1\n        print(\"{}/{} video extracted\".format(processed_count, num_samples), end = \"\\r\")\n        \n    return all_frames, all_labels","metadata":{"execution":{"iopub.status.busy":"2021-10-26T04:05:50.514674Z","iopub.execute_input":"2021-10-26T04:05:50.514987Z","iopub.status.idle":"2021-10-26T04:05:50.524254Z","shell.execute_reply.started":"2021-10-26T04:05:50.514949Z","shell.execute_reply":"2021-10-26T04:05:50.523411Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# train_data, train_labels = prepare_data(train_df.loc[[0,200,300,400]], \"/kaggle/input/ucf101-v2/train/\")\ntrain_data, train_labels = prepare_data(train_df, \"/kaggle/input/ucf101-v2/train/\")","metadata":{"execution":{"iopub.status.busy":"2021-10-26T04:05:52.455702Z","iopub.execute_input":"2021-10-26T04:05:52.455955Z","iopub.status.idle":"2021-10-26T04:06:40.455826Z","shell.execute_reply.started":"2021-10-26T04:05:52.455928Z","shell.execute_reply":"2021-10-26T04:06:40.454973Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_data = np.array(train_data)\ntrain_labels = np.array(train_labels)\nprint(train_data.shape)\nprint(train_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T04:06:40.457389Z","iopub.execute_input":"2021-10-26T04:06:40.458216Z","iopub.status.idle":"2021-10-26T04:06:40.578988Z","shell.execute_reply.started":"2021-10-26T04:06:40.458176Z","shell.execute_reply":"2021-10-26T04:06:40.578220Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"The training data set is then split into the train and test set using train_test_split using 75% of the data for training and the remaining 25% for testing.. The train data is then augmented using the ImageDataGenerator.","metadata":{}},{"cell_type":"code","source":"lb = LabelBinarizer()\ntrain_labels = lb.fit_transform(train_labels)\n\n(trainX, testX, trainY, testY) = train_test_split(train_data, \n                                                  train_labels, \n                                                  test_size=0.25, \n                                                  stratify=train_labels, \n                                                  random_state=21)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T04:06:40.580317Z","iopub.execute_input":"2021-10-26T04:06:40.580721Z","iopub.status.idle":"2021-10-26T04:06:40.713721Z","shell.execute_reply.started":"2021-10-26T04:06:40.580684Z","shell.execute_reply":"2021-10-26T04:06:40.712918Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"trainAug = ImageDataGenerator(rotation_range=30,\n                              zoom_range=0.15,\n                              width_shift_range=0.2,\n                              height_shift_range=0.2,\n                              shear_range=0.15,\n                              horizontal_flip=True,\n                              fill_mode=\"nearest\")\n\nvalAug = ImageDataGenerator()\n\n# define the ImageNet mean subtraction (in RGB order) and set the\n# the mean subtraction value for each of the data augmentation\n# objects\nmean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\ntrainAug.mean = mean\nvalAug.mean = mean","metadata":{"execution":{"iopub.status.busy":"2021-10-26T04:06:40.717623Z","iopub.execute_input":"2021-10-26T04:06:40.717831Z","iopub.status.idle":"2021-10-26T04:06:40.723032Z","shell.execute_reply.started":"2021-10-26T04:06:40.717806Z","shell.execute_reply":"2021-10-26T04:06:40.722352Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Model definition\n\nWe use ResNet50 as the base model without the final layer. The top layers are replaced with a head model, where the parameters are trainable. ","metadata":{}},{"cell_type":"code","source":"baseModel = ResNet50(weights=\"imagenet\", \n                     include_top=False,\n                     input_tensor=Input(shape=(224, 224, 3)))\n\nheadModel = baseModel.output\nheadModel = AveragePooling2D(pool_size=(7, 7))(headModel)\nheadModel = Flatten(name=\"flatten\")(headModel)\nheadModel = Dense(512, activation=\"relu\")(headModel)\nheadModel = Dropout(0.25)(headModel)\nheadModel = Dense(len(lb.classes_), activation=\"softmax\")(headModel)\n\nmodel = Model(inputs=baseModel.input, outputs=headModel)\n\nfor layer in baseModel.layers:\n    layer.trainable = False\n\n# from keras.utils.vis_utils import plot_model\n# plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T04:06:40.724198Z","iopub.execute_input":"2021-10-26T04:06:40.724606Z","iopub.status.idle":"2021-10-26T04:06:45.135429Z","shell.execute_reply.started":"2021-10-26T04:06:40.724570Z","shell.execute_reply":"2021-10-26T04:06:45.134677Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"# compile our model (this needs to be done after our setting our\n# layers to being non-trainable)\nprint(\"[INFO] compiling model...\")\nopt = SGD(learning_rate=1e-4, momentum=0.9, decay=1e-4 / EPOCHS)\n\nmodel.compile(loss=\"categorical_crossentropy\", \n              optimizer=opt,\n              metrics=[\"accuracy\"])\n\nfilepath = \"video_classifier_best.h5\"\nbest_checkpoint = keras.callbacks.ModelCheckpoint(filepath, \n                                                  save_weights_only=False, \n                                                  save_best_only=True, \n                                                  verbose=1)\n\nprint(\"[INFO] training head...\")\nH = model.fit(x = trainAug.flow(trainX, trainY, batch_size = BATCH_SIZE),\n              steps_per_epoch = len(trainX) // BATCH_SIZE,\n              validation_data = valAug.flow(testX, testY),\n              validation_steps = len(testX) // BATCH_SIZE,\n              callbacks=[best_checkpoint],\n              epochs=EPOCHS)\n\nmodel.save('video_classifier.h5', save_format=\"h5\")\n\nf = open('labels.pickle', \"wb\")\nf.write(pickle.dumps(lb))\nf.close()","metadata":{"execution":{"iopub.status.busy":"2021-10-26T04:06:45.136730Z","iopub.execute_input":"2021-10-26T04:06:45.136966Z","iopub.status.idle":"2021-10-26T04:08:48.415141Z","shell.execute_reply.started":"2021-10-26T04:06:45.136936Z","shell.execute_reply":"2021-10-26T04:08:48.414089Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# evaluate the network\nprint(\"[INFO] evaluating network...\")\n\npredictions = model.predict(x = testX.astype(\"float32\"), batch_size = 32)\n\nprint(classification_report(testY.argmax(axis=1),\n                            predictions.argmax(axis=1), \n                            target_names=lb.classes_))\n\n# plot the training loss and accuracy\n#N = args[\"epochs\"]\nN = EPOCHS\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\nplt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy on Dataset\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss/Accuracy\")\nplt.legend(loc=\"lower left\")\nplt.savefig(\"plot.png\")","metadata":{"execution":{"iopub.status.busy":"2021-10-26T04:08:48.416539Z","iopub.execute_input":"2021-10-26T04:08:48.416821Z","iopub.status.idle":"2021-10-26T04:08:51.453468Z","shell.execute_reply.started":"2021-10-26T04:08:48.416776Z","shell.execute_reply":"2021-10-26T04:08:51.452511Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}